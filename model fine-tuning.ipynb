{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca87bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd87deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/bnet/litalv/MLHC/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54aa6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from model.trainning import train_multitask_seq_ae\n",
    "from model.prediction import predict_proba, encode_embeddings, predict_proba_knn, predict_logits, fit_calibrators, predict_proba_calibrated,fit_isotonic_calibrators, predict_isotonic_proba_calibrated\n",
    "from model.utils.eval_results import eval_multitask_from_probs, plot_running_total_subplots, plot_running_total, plot_calibration_curve\n",
    "from model.utils.feature_importance import feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfbf13",
   "metadata": {},
   "source": [
    "# Read dataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242f5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/X_train_seq.pkl\", 'rb') as f:\n",
    "\tX_train = pkl.load(f)\n",
    "with open(f\"data/y_train_lables.pkl\", 'rb') as f:\n",
    "\ty_train = pkl.load(f)\n",
    "with open(f\"data/mask_train.pkl\", 'rb') as f:\n",
    "\tmask_train = pkl.load(f)\n",
    "\n",
    "with open(f\"data/X_test_seq.pkl\", 'rb') as f:\n",
    "\tX_test = pkl.load(f)\n",
    "with open(f\"data/y_test_lables.pkl\", 'rb') as f:\n",
    "\ty_test = pkl.load(f)\n",
    "with open(f\"data/mask_test.pkl\", 'rb') as f:\n",
    "\tmask_test = pkl.load(f)\n",
    "\n",
    "with open(f\"data/X_val_seq.pkl\", 'rb') as f:\n",
    "\tX_val = pkl.load(f)\n",
    "with open(f\"data/y_val_lables.pkl\", 'rb') as f:\n",
    "\ty_val = pkl.load(f)\n",
    "with open(f\"data/mask_val.pkl\", 'rb') as f:\n",
    "\tmask_val = pkl.load(f)\n",
    "\n",
    "with open(f\"data/models_params_dict.pkl\", 'rb') as f:\n",
    "\tmodels_params_dict = pkl.load(f)\n",
    "TASKS = models_params_dict['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae7dd1",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0697d",
   "metadata": {},
   "source": [
    "#### def targets weights using the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b5a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N_pos = y_train.sum(axis=0)\n",
    "train_N_neg = y_train.shape[0] - y_train.sum(axis=0)\n",
    "beta = 0.999\n",
    "\n",
    "weights_bce = train_N_neg / train_N_pos\n",
    "weights_supcon = (1.0 - beta) / (1.0 - (beta ** train_N_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162eddc6",
   "metadata": {},
   "source": [
    "#### running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9ae476",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m r_val \u001b[38;5;241m=\u001b[39m eval_multitask_from_probs(y_val, probs_val, task_names \u001b[38;5;241m=\u001b[39m TASKS, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m r_test \u001b[38;5;241m=\u001b[39m eval_multitask_from_probs(y_test, probs_test, task_names \u001b[38;5;241m=\u001b[39m TASKS, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mi\u001b[49m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEC val prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m r_val:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "model, losses = train_multitask_seq_ae(\n",
    "    X_train, y_train, mask_train,\n",
    "    input_dim=X_train.shape[-1],\n",
    "    batch_size=64,\n",
    "    p_per_task=6, # ensure ≥6 positives per task per batch\n",
    "    epochs=20,\n",
    "    scale_warmup_epochs=1,\n",
    "    lr_warmup_epochs=0,\n",
    "\n",
    "    warmup_scales={'lambda_recon':1.0, 'lambda_bce':0.0, 'lambda_supcon':1.0},\n",
    "    scales={'lambda_recon':0.2, 'lambda_bce':1.0, 'lambda_supcon':2.0},\n",
    "\n",
    "    warmup_lrs={'AE':0, 'BCE':[0, 0, 0], 'SupCon':0},\n",
    "    lrs={'AE':1e-3, 'BCE':[1e-3, 1e-3, 1e-4], 'SupCon':1e-3},\n",
    "\n",
    "    latent_dim=64, \n",
    "    SupCon_latent_dim=32,\n",
    "\n",
    "    pooling_mode =\"mean+max+final\", # \"final\", \"mean+final\", \"mean+max+final\", \"mean+attn\"\n",
    "\n",
    "    weights_bce=weights_bce,\n",
    "    weights_supcon=weights_supcon,\n",
    "    temperature=[0.07,0.07], \n",
    "    supcon_gamma=0.7,\n",
    "    supcon_delta=0.5,\n",
    "    seed=42\n",
    ")\n",
    "# plot_running_total(losses) \n",
    "\n",
    "probs_val = predict_proba(model, X_val, mask_val)  \n",
    "probs_test = predict_proba(model, X_test, mask_test)  \n",
    "r_val = eval_multitask_from_probs(y_val, probs_val, task_names = TASKS, plot=False)\n",
    "r_test = eval_multitask_from_probs(y_test, probs_test, task_names = TASKS, plot=False)\n",
    "\n",
    "print(i)\n",
    "print(\"BEC val prediction\")\n",
    "for i in r_val:\n",
    "    print(i, r_val[i]['roc_auc'], r_val[i]['pr_auc'])\n",
    "print(\"BEC test prediction\")\n",
    "for i in r_test:\n",
    "    print(i, r_test[i]['roc_auc'], r_test[i]['pr_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb3ac8",
   "metadata": {},
   "source": [
    "# Full Fine-tuneinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb334f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_optuna_min.py\n",
    "from typing import Dict, List, Any\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# ---------- metric selector (uses your r_val structure) ----------\n",
    "def make_scorer(metric: str, task_idx=None, tasks=None):\n",
    "    \"\"\"\n",
    "    metric: 'roc' | 'ap'\n",
    "    task_idx: None -> mean over 3 tasks, else 0/1/2 for a single task.\n",
    "    tasks: list of task names (e.g., [\"mortality\",\"prolonged_stay\",\"readmission\"])\n",
    "    \"\"\"\n",
    "    key = \"roc_auc\" if metric == \"roc\" else \"pr_auc\"\n",
    "\n",
    "    def score_fn(r_val):\n",
    "        # r_val may be a dict keyed by names OR a list indexed by 0..2\n",
    "        if isinstance(r_val, dict):\n",
    "            # dict keyed by task names\n",
    "            if task_idx is None:\n",
    "                names = tasks or list(r_val.keys())\n",
    "                return float(np.mean([r_val[name][key] for name in names]))\n",
    "            else:\n",
    "                name = (tasks[task_idx] if tasks else task_idx)\n",
    "                return float(r_val[name][key])\n",
    "        else:\n",
    "            # list/tuple indexed by 0..2\n",
    "            if task_idx is None:\n",
    "                return float(np.mean([r_val[i][key] for i in range(3)]))\n",
    "            else:\n",
    "                return float(r_val[task_idx][key])\n",
    "    return score_fn\n",
    "\n",
    "# ---------- one train→eval using your API ----------\n",
    "def run_once(params, data, epochs_for_tuning = 12, seed = 42):\n",
    "    model, _ = train_multitask_seq_ae(\n",
    "        data[\"X_train\"], data[\"y_train\"], data[\"mask_train\"],\n",
    "        input_dim=data[\"X_train\"].shape[-1],\n",
    "\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        p_per_task=params[\"p_per_task\"],\n",
    "        epochs=epochs_for_tuning,\n",
    "\n",
    "        warmup_epochs=params[\"warmup_epochs\"],\n",
    "        warmup_scales=params[\"warmup_scales\"],\n",
    "\n",
    "        max_lr=params[\"max_lr\"],\n",
    "        min_lr=params[\"min_lr\"],\n",
    "\n",
    "        latent_dim=params[\"latent_dim\"],\n",
    "        SupCon_latent_dim=params[\"SupCon_latent_dim\"],\n",
    "        pooling_mode=params[\"pooling_mode\"],\n",
    "\n",
    "        lambda_recon=params[\"lambda_recon\"],\n",
    "        lambda_bce=params[\"lambda_bce\"],\n",
    "        lambda_supcon=params[\"lambda_supcon\"],\n",
    "\n",
    "        weights_bce=data[\"weights_bce\"],\n",
    "        weights_supcon=data[\"weights_supcon\"],\n",
    "\n",
    "        temperature=params[\"temperature\"],\n",
    "        supcon_gamma=params[\"supcon_gamma\"],\n",
    "        supcon_delta=params[\"supcon_delta\"],\n",
    "        seed=seed,\n",
    "    )\n",
    "    probs_val = predict_proba(model, data[\"X_val\"], data[\"mask_val\"])\n",
    "    r_val = eval_multitask_from_probs(data[\"y_val\"], probs_val, task_names=data[\"TASKS\"], plot=False)\n",
    "    return r_val\n",
    "\n",
    "# ---------- minimal Optuna runner ----------\n",
    "def optuna_search(\n",
    "    data: Dict[str, Any],\n",
    "    base_params: Dict[str, Any],\n",
    "    spaces: Dict[str, List[Any]],   # discrete candidates per param (categorical only)\n",
    "    metric: str = \"roc\",            # 'roc' | 'ap'\n",
    "    task_idx = None,    # 0/1/2 for a single task; None = mean across tasks\n",
    "    n_trials: int = 20,\n",
    "    epochs_for_tuning: int = 12,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\" choose *exact* values to test in `spaces`. \n",
    "        anything not listed uses `base_params`. \"\"\"\n",
    "    scorer = make_scorer(metric, task_idx, tasks=data.get(\"TASKS\"))\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=seed))\n",
    "\n",
    "    def objective(trial: optuna.Trial):\n",
    "        params = dict(base_params)\n",
    "        for name, candidates in spaces.items():\n",
    "            idx = trial.suggest_categorical(f\"{name}_idx\", list(range(len(candidates))))\n",
    "            params[name] = candidates[idx]\n",
    "        r_val = run_once(params, data, epochs_for_tuning=epochs_for_tuning, seed=seed + trial.number)\n",
    "        return scorer(r_val)\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    # Rebuild best params (resolve *_idx → actual value)\n",
    "    best_params = dict(base_params)\n",
    "    for name, candidates in spaces.items():\n",
    "        best_idx = study.best_trial.params[f\"{name}_idx\"]\n",
    "        best_params[name] = candidates[int(best_idx)]\n",
    "\n",
    "    # Short confirm run (optional)\n",
    "    r_val_best = run_once(best_params, data, epochs_for_tuning=epochs_for_tuning, seed=seed)\n",
    "    print(\"\\nBest score:\", scorer(r_val_best))\n",
    "    print(\"Best params:\", {k: best_params[k] for k in sorted(best_params)})\n",
    "    print(\"Best r_val:\", r_val_best)\n",
    "    return best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    X_train=X_train, y_train=y_train, mask_train=mask_train,\n",
    "    X_val=X_val,     y_val=y_val,     mask_val=mask_val,\n",
    "    weights_bce=weights_bce, weights_supcon=weights_supcon,\n",
    "    TASKS=TASKS,\n",
    ")\n",
    "\n",
    "# Baseline (anything not varied in 'spaces' stays as-is)\n",
    "base = dict(\n",
    "    batch_size=96,\n",
    "    p_per_task=4,\n",
    "    warmup_epochs=0,\n",
    "    warmup_scales=(0.0, 0.0, 0.0),\n",
    "\n",
    "    max_lr=5e-4,\n",
    "    min_lr=1e-4,\n",
    "\n",
    "    latent_dim=80,\n",
    "    SupCon_latent_dim=16,\n",
    "    pooling_mode=\"mean+final\",\n",
    "\n",
    "    lambda_recon=0.3,\n",
    "    lambda_bce=1.0,\n",
    "    lambda_supcon=1.0,\n",
    "\n",
    "    temperature=0.0953,\n",
    "    supcon_gamma=0.7,\n",
    "    supcon_delta=0.5,\n",
    ")\n",
    "\n",
    "# Choose exactly what to vary (add/remove freely)\n",
    "spaces = {\n",
    "    # batching / sampling\n",
    "    \"batch_size\": [64, 96, 128],\n",
    "    \"p_per_task\": [4, 6, 8, 10],\n",
    "\n",
    "    # warmup & LR\n",
    "    \"warmup_epochs\": [0, 1, 2],\n",
    "    \"warmup_scales\": [(0.0, 0.0, 1.0), (0.0, 0.5, 1.0), (0.5, 0.0, 2.0)],\n",
    "    \"max_lr\": [5e-4, 7.5e-4, 1e-3],\n",
    "    \"min_lr\": [5e-5, 1e-4, 2e-4],\n",
    "\n",
    "    # capacity / pooling\n",
    "    \"latent_dim\": [64, 80, 96, 128],\n",
    "    \"SupCon_latent_dim\": [16, 32, 48, 64],\n",
    "    \"pooling_mode\": [\"mean+final\", \"max+final\", \"mean+max+final\"],\n",
    "\n",
    "    # losses & SupCon\n",
    "    \"lambda_recon\": [0.1, 0.2, 0.3],\n",
    "    \"lambda_bce\": [0.8, 1.0, 1.2],\n",
    "    \"lambda_supcon\": [1.0, 2.0, 3.0],\n",
    "    \"temperature\": [0.06, 0.08, 0.10, 0.12],\n",
    "    # \"supcon_gamma\": [0.6, 0.7, 0.8],\n",
    "    # \"supcon_delta\": [0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "# Optimize MEAN AP across 3 tasks\n",
    "best_mean_ap, study_mean_ap = optuna_search(\n",
    "    data, base, spaces,\n",
    "    metric=\"ap\", task_idx=None,\n",
    "    n_trials=200, epochs_for_tuning=12, seed=42\n",
    ")\n",
    "\n",
    "best_task_roc, study_task_roc, best_task_ap, study_task_ap = [],[],[],[]\n",
    "for i in range(3):\n",
    "    # Optimize ROC for a single task (e.g., readmission = task 2)\n",
    "    best_task2_roc, study_task2_roc = optuna_search(\n",
    "        data, base, spaces,\n",
    "        metric=\"roc\", task_idx=i,\n",
    "        n_trials=24, epochs_for_tuning=12, seed=42\n",
    "    )\n",
    "    best_task_roc.append(best_task2_roc)\n",
    "    study_task_roc.append(study_task2_roc)\n",
    "\n",
    "    # Optimize AP for the same task\n",
    "    best_task2_ap, study_task2_ap = optuna_search(\n",
    "        data, base, spaces,\n",
    "        metric=\"ap\", task_idx=i,\n",
    "        n_trials=24, epochs_for_tuning=12, seed=42\n",
    "    )\n",
    "    best_task_ap.append(best_task2_ap)\n",
    "    study_task_ap.append(study_task2_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc5c74",
   "metadata": {},
   "source": [
    "# Weights scaling fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Objective to maximize ---\n",
    "def objective(trial):\n",
    "    data = dict(\n",
    "        X_train=X_train, y_train=y_train, mask_train=mask_train,\n",
    "        X_val=X_val,     y_val=y_val,     mask_val=mask_val,\n",
    "        weights_bce=weights_bce, weights_supcon=weights_supcon,\n",
    "        TASKS=TASKS,\n",
    "    )\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    supcon_gamma          = trial.suggest_float(\"supcon_gamma\", 0.2, 0.7)\n",
    "    supcon_delta       = trial.suggest_float(\"supcon_delta\", 0.2, 0.7, log=True)\n",
    "\n",
    "    # Train with suggested params\n",
    "    model, _ = train_multitask_seq_ae(\n",
    "        data[\"X_train\"], data[\"y_train\"], data[\"mask_train\"],\n",
    "        input_dim=data[\"X_train\"].shape[-1],\n",
    "\n",
    "        batch_size=64,\n",
    "        p_per_task=6,\n",
    "        epochs=20,\n",
    "\n",
    "        warmup_epochs=0,\n",
    "        warmup_scales=(0.0, 0.0, 1.0),\n",
    "\n",
    "        max_lr=5e-4,\n",
    "        min_lr=1e-4,\n",
    "\n",
    "        latent_dim=80,\n",
    "        SupCon_latent_dim=16,\n",
    "        pooling_mode=\"mean+final\",\n",
    "\n",
    "        lambda_recon=0.3,\n",
    "        lambda_bce=1,\n",
    "        lambda_supcon=1,\n",
    "\n",
    "        weights_bce=data[\"weights_bce\"],\n",
    "        weights_supcon=data[\"weights_supcon\"],\n",
    "\n",
    "        temperature=[0.12,0.095],\n",
    "        supcon_gamma=supcon_gamma,\n",
    "        supcon_delta=supcon_delta,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation using ROC-AUC (mean of available tasks)\n",
    "    probs_val = predict_proba(model, X_val, mask_val)   # shape: (N, 3)\n",
    "    r_val = eval_multitask_from_probs(y_val, probs_val, task_names = TASKS, plot=False)\n",
    "    mean_auc = float(np.mean([r_val[i]['roc_auc'] for i in r_val]))\n",
    "\n",
    "    return mean_auc  # Optuna will MAXIMIZE this\n",
    "\n",
    "# --- Run the search ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, n_jobs=1)  # increase n_trials as you like\n",
    "\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8b762",
   "metadata": {},
   "source": [
    "# compare configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c50c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    X_train=X_train, y_train=y_train, mask_train=mask_train,\n",
    "    X_val=X_val,     y_val=y_val,     mask_val=mask_val,\n",
    "    weights_bce=weights_bce, weights_supcon=weights_supcon,\n",
    "    TASKS=TASKS,\n",
    ")\n",
    "\n",
    "# ===== Suggested configs =====\n",
    "config_names = [ \"baseline\",\n",
    "  \"M1_lambdaSupCon_0.8\",\n",
    "  \"M2_temp_0.10\",\n",
    "  \"M3_supconLatent_32\",\n",
    "  \"M4_pPerTask_6\",\n",
    "  \"M5_longer_train_lower_minLR\",\n",
    "  \"M6_light_warmup_bce_first\"\n",
    "]\n",
    "\n",
    "baseline_config = dict(\n",
    "    batch_size=96,\n",
    "    p_per_task=4,\n",
    "    warmup_epochs=0,\n",
    "    warmup_scales=(0.0, 0.0, 1.0),\n",
    "\n",
    "    max_lr=5e-4,\n",
    "    min_lr=1e-4,\n",
    "\n",
    "    latent_dim=80,\n",
    "    SupCon_latent_dim=16,\n",
    "    pooling_mode=\"mean+final\",\n",
    "\n",
    "    lambda_recon=0.3,\n",
    "    lambda_bce=1.0,\n",
    "    lambda_supcon=1.0,\n",
    "\n",
    "    temperature=0.09533360660422997,\n",
    "    supcon_gamma=0.7,\n",
    "    supcon_delta=0.5,\n",
    ")\n",
    "\n",
    "configs = [\n",
    "  # M1: Reduce SupCon dominance slightly\n",
    "  dict(batch_size=96, p_per_task=4, warmup_epochs=0, warmup_scales=(0.0,0.0,1.0),\n",
    "       max_lr=5e-4, min_lr=1e-4,\n",
    "       latent_dim=80, SupCon_latent_dim=16, pooling_mode=\"mean+final\",\n",
    "       lambda_recon=0.3, lambda_bce=1.0, lambda_supcon=0.8,\n",
    "       temperature=0.0953, supcon_gamma=0.7, supcon_delta=0.5),\n",
    "\n",
    "  # M2: Softer SupCon via temperature\n",
    "  dict(batch_size=96, p_per_task=4, warmup_epochs=0, warmup_scales=(0.0,0.0,1.0),\n",
    "       max_lr=5e-4, min_lr=1e-4,\n",
    "       latent_dim=80, SupCon_latent_dim=16, pooling_mode=\"mean+final\",\n",
    "       lambda_recon=0.3, lambda_bce=1.0, lambda_supcon=1.0,\n",
    "       temperature=0.10, supcon_gamma=0.7, supcon_delta=0.5),\n",
    "\n",
    "  # M3: Slightly larger projection (can help AP without cranking λ_supcon)\n",
    "  dict(batch_size=96, p_per_task=4, warmup_epochs=0, warmup_scales=(0.0,0.0,1.0),\n",
    "       max_lr=5e-4, min_lr=1e-4,\n",
    "       latent_dim=80, SupCon_latent_dim=32, pooling_mode=\"mean+final\",\n",
    "       lambda_recon=0.3, lambda_bce=1.0, lambda_supcon=1.0,\n",
    "       temperature=0.0953, supcon_gamma=0.7, supcon_delta=0.5),\n",
    "\n",
    "  # M4: A touch more positives per task (you saw 8–10 hurt; try just 6)\n",
    "  dict(batch_size=96, p_per_task=6, warmup_epochs=0, warmup_scales=(0.0,0.0,1.0),\n",
    "       max_lr=5e-4, min_lr=1e-4,\n",
    "       latent_dim=80, SupCon_latent_dim=16, pooling_mode=\"mean+final\",\n",
    "       lambda_recon=0.3, lambda_bce=1.0, lambda_supcon=1.0,\n",
    "       temperature=0.0953, supcon_gamma=0.7, supcon_delta=0.5),\n",
    "\n",
    "  # M5: Let BCE descend more — longer run & smaller tail LR\n",
    "  dict(batch_size=96, p_per_task=4, warmup_epochs=0, warmup_scales=(0.0,0.0,1.0),\n",
    "       max_lr=5e-4, min_lr=5e-5,  # ↓ tail LR\n",
    "       latent_dim=80, SupCon_latent_dim=16, pooling_mode=\"mean+final\",\n",
    "       lambda_recon=0.3, lambda_bce=1.0, lambda_supcon=1.0,\n",
    "       temperature=0.0953, supcon_gamma=0.7, supcon_delta=0.5,\n",
    "       # run this one for 25–30 epochs in your loop\n",
    "  ),\n",
    "\n",
    "  # M6: Give BCE a head start (very light warmup)\n",
    "  dict(batch_size=96, p_per_task=4, warmup_epochs=1, warmup_scales=(0.0,0.5,1.0),\n",
    "       max_lr=5e-4, min_lr=1e-4,\n",
    "       latent_dim=80, SupCon_latent_dim=16, pooling_mode=\"mean+final\",\n",
    "       lambda_recon=0.3, lambda_bce=1.0, lambda_supcon=1.0,\n",
    "       temperature=0.0953, supcon_gamma=0.7, supcon_delta=0.5),\n",
    "]\n",
    "\n",
    "for c, conf in enumerate([baseline_config]+configs):\n",
    "    print(f\"\\n=== {c}: {config_names[c]} ===\")\n",
    "    model, _ = train_multitask_seq_ae(\n",
    "        data[\"X_train\"], data[\"y_train\"], data[\"mask_train\"],\n",
    "        input_dim=data[\"X_train\"].shape[-1],\n",
    "\n",
    "        batch_size=conf[\"batch_size\"],\n",
    "        p_per_task=conf[\"p_per_task\"],\n",
    "        epochs=20,\n",
    "\n",
    "        warmup_epochs=conf[\"warmup_epochs\"],\n",
    "        warmup_scales=conf[\"warmup_scales\"],\n",
    "\n",
    "        max_lr=conf[\"max_lr\"],\n",
    "        min_lr=conf[\"min_lr\"],\n",
    "\n",
    "        latent_dim=conf[\"latent_dim\"],\n",
    "        SupCon_latent_dim=conf[\"SupCon_latent_dim\"],\n",
    "        pooling_mode=conf[\"pooling_mode\"],\n",
    "\n",
    "        lambda_recon=conf[\"lambda_recon\"],\n",
    "        lambda_bce=conf[\"lambda_bce\"],\n",
    "        lambda_supcon=conf[\"lambda_supcon\"],\n",
    "\n",
    "        weights_bce=data[\"weights_bce\"],\n",
    "        weights_supcon=data[\"weights_supcon\"],\n",
    "\n",
    "        temperature=conf[\"temperature\"],\n",
    "        supcon_gamma=conf[\"supcon_gamma\"],\n",
    "        supcon_delta=conf[\"supcon_delta\"],\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    probs_val = predict_proba(model, X_val, mask_val)  \n",
    "    probs_test = predict_proba(model, X_test, mask_test)  \n",
    "    r_val = eval_multitask_from_probs(y_val, probs_val, task_names = TASKS, plot=False)\n",
    "    r_test = eval_multitask_from_probs(y_test, probs_test, task_names = TASKS, plot=False)\n",
    "\n",
    "    print(\"BEC val prediction\")\n",
    "    for i in r_val:\n",
    "        print(i, r_val[i]['roc_auc'], r_val[i]['pr_auc'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f060d20",
   "metadata": {},
   "source": [
    "# Eval each models part contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7a7ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "BASE_ARGS = dict(\n",
    "    batch_size=96,\n",
    "    p_per_task=4,\n",
    "    warmup_epochs=0,\n",
    "    warmup_scales=(0.0, 0.0, 1.0),\n",
    "\n",
    "    max_lr=5e-4,\n",
    "    min_lr=1e-4,\n",
    "\n",
    "    latent_dim=80,\n",
    "    SupCon_latent_dim=16,\n",
    "    pooling_mode=\"mean+final\",\n",
    "\n",
    "    lambda_recon=0.3,\n",
    "    lambda_bce=1.0,\n",
    "    lambda_supcon=1.0,\n",
    "\n",
    "    temperature=0.09533360660422997,\n",
    "    supcon_gamma=0.7,\n",
    "    supcon_delta=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ABLATIONS = {\n",
    "    # baseline (all losses on)\n",
    "    \"baseline\": dict(lambda_recon=0.2, lambda_bce=1.0, lambda_supcon=1.0, warmup_epochs=0),\n",
    "    # remove SupCon term only\n",
    "    \"no_supcon\": dict(lambda_recon=0.2, lambda_bce=1.0, lambda_supcon=0.0, warmup_epochs=0),\n",
    "    # remove Recon term only\n",
    "    \"no_recon\": dict(lambda_recon=0.0, lambda_bce=1.0, lambda_supcon=1.0, warmup_epochs=0),\n",
    "    # BCE only (both Recon and SupCon off)\n",
    "    \"bce_only\": dict(lambda_recon=0.0, lambda_bce=1.0, lambda_supcon=0.0, warmup_epochs=0),\n",
    "    # SupCon only (heads won’t train; use KNN for eval)\n",
    "    \"supcon_only\": dict(lambda_recon=0.0, lambda_bce=0.0, lambda_supcon=1.0, warmup_epochs=0),\n",
    "}\n",
    "\n",
    "def run_one(name, xtr, ytr, mtr, xte, yte, mte):\n",
    "    # merge base args + ablation-specific overrides\n",
    "    args = copy.deepcopy(BASE_ARGS)\n",
    "    args.update(ABLATIONS[name])\n",
    "    # train\n",
    "    model, _ = train_multitask_seq_ae(\n",
    "        xtr, ytr, mtr,\n",
    "        input_dim=xtr.shape[-1],\n",
    "        weights_bce=weights_bce,\n",
    "        weights_supcon=weights_supcon,\n",
    "        **args\n",
    "    )\n",
    "    # eval\n",
    "    out = {\"setting\": name}\n",
    "\n",
    "    # BCE probabilities (only meaningful if lambda_bce>0)\n",
    "    if args[\"lambda_bce\"] > 0.0:\n",
    "        probs_bce = predict_proba(model, xte, mte)\n",
    "        rep_bce = eval_multitask_from_probs(yte, probs_bce, plot=False, task_names=TASKS)\n",
    "        for t in TASKS:\n",
    "            out[f\"{t}_ROC_bce\"] = rep_bce[t][\"roc_auc\"]\n",
    "            out[f\"{t}_PR_bce\"]  = rep_bce[t][\"pr_auc\"]\n",
    "    else:\n",
    "        for t in TASKS:\n",
    "            out[f\"{t}_ROC_bce\"] = None\n",
    "            out[f\"{t}_PR_bce\"]  = None\n",
    "\n",
    "    # KNN over embeddings (meaningful for all; *primary* for supcon_only)\n",
    "    probs_knn = predict_proba_knn(model, xtr, xte, mtr, mte, ytr, n_neig=10)\n",
    "    rep_knn = eval_multitask_from_probs(yte, probs_knn, plot=False, task_names=TASKS)\n",
    "    for t in TASKS:\n",
    "        out[f\"{t}_ROC_knn\"] = rep_knn[t][\"roc_auc\"]\n",
    "        out[f\"{t}_PR_knn\"]  = rep_knn[t][\"pr_auc\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==== run all ====\n",
    "rows = []\n",
    "for name in ABLATIONS:\n",
    "    rows.append(run_one(name, X_train, y_train, mask_train, X_test, y_test, mask_test))\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# pretty order\n",
    "cols = [\"setting\"]\n",
    "for src in [\"bce\",\"knn\"]:\n",
    "    for t in TASKS:\n",
    "        cols += [f\"{t}_ROC_{src}\", f\"{t}_PR_{src}\"]\n",
    "results_df = results_df[cols]\n",
    "\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e1c9e",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88925a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model, \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
